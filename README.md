# Testing LLMs as a Rubik's Cube Tutor
In this repo, I experimented with prompting LLMs, specifically gpt4.0 and looked at how they generated different responses as an AI tutor
teaching me how to solve a rubik's cube.
When using vanilla prompting or just live thoughts and inputs, the LLM produced lengthy and quite thorough responses, just spewing out information.
However, using a more descriptive prompt, like Molick's Tutoring Prompt, the LLM was more simple and helped aided me with ensuring that I understood the concepts, not just reading how to do it. This interactive part I think was much more effective in actually undersatnding what was being "taught" by the AI tutor, showcasing the power of meaningful prompting.