# Testing LLMs as a Rubik's Cube Tutor
In this repo, I experimented with prompting LLMs, specifically gpt4.0 and looked at how they generated different responses as an AI tutor
teaching me how to solve a rubik's cube.
When using vanilla prompting or just live thoughts and inputs, the LLM produced lengthy and quite thorough responses, just spewing out information. However, using a more descriptive prompt, like Molick's Tutoring Prompt, the LLM was more simple and helped aided me with ensuring that I understood the concepts, not just reading how to do it. This interactive part I think was much more effective in actually undersatnding what was being "taught" by the AI tutor, showcasing the power of meaningful prompting. I think what I enjoyed most however, was when I personalized Molik's tutoring prompt so the LLM would produce responses that I was looking for. When I felt unmotivated, the AI tutor did not push me to continue giong, but rather stay wihtin my comfort zone. It was interesting to see how the AI was able to evaluate progess even with a skill that required hands-on experimentation that the AI couldn't actually visibly see what I was doing.